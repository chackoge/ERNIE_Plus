import json
import subprocess
from pathlib import Path

import click
import matplotlib.pyplot as plt
import numpy as np
import psycopg2

from python_scripts.cluster_processing_scripts import convert_to_cluster_id_format

SMALL_SIZE = 12
MEDIUM_SIZE = 18
BIGGER_SIZE = 24

plt.rc("font", size=SMALL_SIZE)          # controls default text sizes
plt.rc("axes", titlesize=SMALL_SIZE)     # fontsize of the axes title
plt.rc("axes", labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels
plt.rc("xtick", labelsize=SMALL_SIZE)    # fontsize of the tick labels
plt.rc("ytick", labelsize=SMALL_SIZE)    # fontsize of the tick labels
plt.rc("legend", fontsize=SMALL_SIZE)    # legend fontsize
plt.rc("figure", titlesize=BIGGER_SIZE)


def mapping_to_dict(mapping):
    '''This function takes in a mapping generated by create_leiden_input
    where each row is "<node id>SPACE<True id>" and returns dicts going
    both ways
    '''
    integer_to_id_dict = {}
    id_to_integer_dict = {}
    with open(mapping, "r") as f:
        for line in f:
            node_id,true_id = line.split()
            integer_to_id_dict[int(node_id)] = true_id
            id_to_integer_dict[true_id] = int(node_id)
    return {
        "integer_to_id_dict": integer_to_id_dict,
        "id_to_integer_dict": id_to_integer_dict,
    }


def inverse_mapping_to_dict(mapping):
    '''This function takes in a inverse mapping generated by networkit
    where each row is "<new id>SPACE<old id>"
    '''
    new_to_old_dict = {}
    old_to_new_dict = {}
    with open(mapping, "r") as f:
        for line in f:
            new_id,old_id = line.split()
            new_to_old_dict[new_id] = old_id
            old_to_new_dict[old_id] = new_id
    return {
        "new_to_old_dict": new_to_old_dict,
        "old_to_new_dict": old_to_new_dict,
    }


def file_to_dict(clustering):
    '''This function takes in an input clustering of format "<cluster number>SPACE<node id>"
    and returns two python dictionaries that map from cluster number to a list of node ids and
    from a node id to list of cluster numbers.
    '''
    cluster_to_id_dict = {}
    id_to_cluster_dict = {}

    with open(clustering, "r") as f:
        for current_line in f:
            [current_cluster_number, id] = current_line.strip().split()
            if(int(current_cluster_number) not in cluster_to_id_dict):
                cluster_to_id_dict[int(current_cluster_number)] = []
            if(id not in id_to_cluster_dict):
                id_to_cluster_dict[id] = []
            cluster_to_id_dict[int(current_cluster_number)].append(id)
            id_to_cluster_dict[id].append(int(current_cluster_number))
    for current_id in id_to_cluster_dict:
        id_to_cluster_dict[current_id] = list(set(id_to_cluster_dict[current_id]))
    return {
        "cluster_to_id_dict": cluster_to_id_dict,
        "id_to_cluster_dict": id_to_cluster_dict,
    }

def write_new_sorted_cluster_dict(to_be_updated_dict, unassigned_nodes, output_prefix, mapping_file=None):
    '''This function takes in a dictionary of cluster_ids to ids and writes a clustering file
    where each line is "<cluster number>SPACE<node id>" and is sorted by cluster size of each cluster number
    '''
    sorted_list = sorted(list(to_be_updated_dict.items()), key=lambda tup:len(tup[1]), reverse=True)
    sorted_list = list(filter(lambda tup: len(tup[1]) > 0, sorted_list))
    remapped_list = sorted_list
    if (mapping_file):
        inverse_mapping = inverse_mapping_to_dict(mapping_file)["new_to_old_dict"]
        remapped_list = []
        for original_cluster_id,cluster_members in sorted_list:
            remapped_list.append((original_cluster_id, [inverse_mapping[node_id] for node_id in cluster_members]))

    '''Unnecessary since it only causes confusion
    renumbered_map = {}
    '''
    with open(f"{output_prefix}.clustering", "w") as f_clustering:
        with open(f"{output_prefix}.summary", "w") as f_summary:
            for sequential_id,(original_cluster_id, cluster_members) in enumerate(remapped_list):
                '''Unnecessary since it only causes confusion
                renumbered_map[original_cluster_id] = sequential_id
                '''
                for cluster_member in cluster_members:
                    f_clustering.write(f"{sequential_id} {cluster_member}\n")
                f_summary.write(f"{sequential_id} {len(cluster_members)}\n")
            for unassigned_node_index,unassigned_node in enumerate(unassigned_nodes):
                f_clustering.write(f"{len(remapped_list) + unassigned_node_index} {unassigned_node}\n")
                f_summary.write(f"{len(remapped_list) + unassigned_node_index} 1\n")

    with open(f"{output_prefix}.raw.clustering", "w") as f_raw_clustering:
        for original_cluster_id,cluster_members in remapped_list:
            for cluster_member in cluster_members:
                f_raw_clustering.write(f"{original_cluster_id} {cluster_member}\n")

    '''Unnecessary since it only causes confusion
    with open(f"{output_prefix}.renumbered.mapping", "w") as f_mapping:
        for original_cluster_id,sequential_id in renumbered_map.items():
            f_mapping.write(f"{original_cluster_id} {sequential_id}\n")
    '''


def run_leiden(input_network, resolution, output_prefix):
    output_raw_cluster_file = f"{output_prefix}_leiden_{resolution}.raw.clustering"
    output_processed_cluster_file = f"{output_prefix}_leiden_{resolution}.clustering"
    with open(f"{output_prefix}_leiden_{resolution}.err", "w") as f_err:
        with open(f"{output_prefix}_leiden_{resolution}.out", "w") as f_out:
            subprocess.run(["/usr/bin/time", "-v", "java", "-cp", "/srv/local/shared/external/leiden/networkanalysis-1.1.0/leiden.jar", "nl.cwts.networkanalysis.run.RunNetworkClustering", "-r", str(resolution), "-o", output_raw_cluster_file, input_network], stdout=f_out, stderr=f_err)
    convert_to_cluster_id_format.parse_leiden(output_raw_cluster_file, None, output_processed_cluster_file)
    return file_to_dict(output_processed_cluster_file)["cluster_to_id_dict"]


def run_graclus(input_network, num_clusters, output_prefix):
    output_raw_cluster_file = f"{output_prefix}_graclus.raw.clustering"
    output_processed_cluster_file = f"{output_prefix}_graclus.clustering"
    with open(f"{output_prefix}_graclus.err", "w") as f_err:
        with open(f"{output_prefix}_graclus.out", "w") as f_out:
            subprocess.run(["/usr/bin/time", "-v", "/srv/shared/external/graclus1.2/graclus", "-l", "2000", input_network, str(num_clusters)], stdout=f_out, stderr=f_err)
    output_file_base_name = Path(input_network).name
    subprocess.run(["mv", f"./{output_file_base_name}.part.{num_clusters}", output_raw_cluster_file])
    convert_to_cluster_id_format.parse_graclus(output_raw_cluster_file, output_processed_cluster_file)
    return file_to_dict(output_processed_cluster_file)["cluster_to_id_dict"]


def run_ikc(compacted_graph_filename, k, b, output_prefix):
    output_raw_cluster_file = f"{output_prefix}/ikc.raw.clustering"
    output_processed_cluster_file = f"{output_prefix}/ikc.clustering"
    with open(f"{output_prefix}/ikc_k_{k}_b_{b}.err", "w") as f_err:
        with open(f"{output_prefix}/ikc_k_{k}_b_{b}.out", "w") as f_out:
            subprocess.run(["/usr/bin/time", "-v", "python", "/srv/shared/external/for_eleanor/IKC_MCS_ES.py", "-e", compacted_graph_filename, "-k", str(k), "-b", str(b), "-o", output_raw_cluster_file], stdout=f_out, stderr=f_err)
    convert_to_cluster_id_format.parse_ikc(output_raw_cluster_file, output_processed_cluster_file)
    return file_to_dict(output_processed_cluster_file)["cluster_to_id_dict"]


def run_mcl(input_network, inflation, output_prefix):
    cluster_to_id_dict = {}
    output_mci = f"{output_prefix}/mcl.{str(inflation)}.mci"
    output_tab = f"{output_prefix}/mcl.{str(inflation)}.tab"
    output_mcl = f"{output_prefix}/mcl.{str(inflation)}.mcl"
    output_dump = f"{output_prefix}/dump.mcl.{str(inflation)}.mcl"
    output_processed_cluster_file = f"{output_prefix}/mcl.{str(inflation)}.clustering"
    with open(f"{output_prefix}/mcl_{inflation}.err", "w") as f_err:
        with open(f"{output_prefix}/mcl_{inflation}.out", "w") as f_out:
            subprocess.run(["/usr/bin/time", "-v", "/srv/shared/external/mcl-14-137/bin/mcxload", "--stream-mirror", "-abc", input_network, "-o", output_mci, "-write-tab", output_tab], stdout=f_out, stderr=f_err)
            subprocess.run(["/usr/bin/time", "-v", "/srv/shared/external/mcl-14-137/bin/mcl", output_mci, "-I", inflation, "-o", output_mcl], stdout=f_out, stderr=f_err)

            subprocess.run(["/usr/bin/time", "-v", "/srv/shared/external/mcl-14-137/bin/mcxdump", "-icl", output_mcl, "-tabr", output_tab, "-o", output_dump], stdout=f_out, stderr=f_err)
            subprocess.run(["Rscript", "/home/minhyuk2/git_repos/ERNIE_Plus/Illinois/clustering/minhyuk/mcl_scripts/post_process.R"], stdout=f_out, stderr=f_err) # this takes in all files that match dump.*
    convert_to_cluster_id_format.parse_mcl(f"{output_dump}.csv", output_processed_cluster_file)
    return file_to_dict(output_processed_cluster_file)["cluster_to_id_dict"]


def save_histogram(x_min, x_max, bin_size, data, y_label, x_label, title, output_prefix):
    plt.clf()
    plt.figure(figsize=(20, 17))
    bins = np.arange(x_min, x_max, bin_size)
    # plt.locator_params(nbins=10)
    plt.xticks(np.linspace(x_min, x_max, 25, dtype=int))
    plt.hist(data, bins=bins, density=False)
    plt.ylabel(y_label)
    plt.xlabel(x_label);
    plt.title(title);
    plt.savefig(output_prefix + ".png", bbox_inches="tight")
    plt.close()


def save_scatter(x_data, y_data, x_label, y_label, title, output_prefix, y_min=0, add_x_y_line=False, log_log=False):
    # plt.locator_params(nbins=10)
    plt.clf()
    plt.figure(figsize=(20, 17))
    plt.xticks(np.linspace(0, max(x_data), 25, dtype=int))
    plt.scatter(x_data, y_data)
    if(add_x_y_line):
        print("adding y=x")
        plt.plot(x_data, x_data, color="red", label="y=x")
    plt.ylim(ymin=y_min)
    plt.xlabel(x_label)
    plt.ylabel(y_label)
    plt.title(title)
    plt.savefig(output_prefix + ".png", bbox_inches="tight")
    plt.close()
